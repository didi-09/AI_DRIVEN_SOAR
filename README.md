# DIDI RL SOAR V5: "Deep Eyes" Technical Whitepaper üõ°Ô∏èüß†

> [!IMPORTANT]
> **This is the Definitive Engineering Specification.**
> It details the V5 Architecture, Distributed Training Pipeline, Component Mechanics, and Validated Performance Metrics.

---

## 1. Executive Summary: The Autonomous Cyber-Reactions System

DIDI RL SOAR is a closed-loop **Autonomous Cyber-Defense System**. It replaces static playbooks with a dynamic, learning agent capable of reasoning through complex, multi-stage attacks.

### The System at a Glance
*   **The Problem**: Traditional SOAR tools (Splunk Phantom, XSOAR) run static scripts. They cannot adapt to novel attacks or balance "Security vs. Business Continuity."
*   **The Solution**: A **Reinforcement Learning (PPO)** agent that learns optimal defense strategies by training against a hyper-realistic simulator.
*   **Current State (V5)**: The system has achieved **Production Mastery** (Risk Delta > 0.94) using a Distributed "Deep Eyes" architecture.

---

## 2. System Architecture: The V5 Loop üåê

The V5 architecture is a **Distributed Reinforcement Learning System** designed to solve the "Training Speed vs. VRAM" bottleneck. It decouples the Simulation Environment (CPU) from the Learning Core (GPU) using a specialized **Spawn-Based Multiprocessing Pool**.

### 2.1 High-Level Architecture Diagram
```mermaid
graph TD
    subgraph "The Brain (GPU - Central Learner)"
        Obs_Net["Observer Net (Deep Eyes)"]
        Agent_Net["PPO Agent (512 Units)"]
        Replay["Global Replay Buffer"]
        TPE["Optuna Optimization Kernel"]
    end

    subgraph "The Body (CPU - Distributed Pool)"
        Worker1["Worker 1: CyberRangeEnv"]
        Worker2["Worker 2: CyberRangeEnv"]
        Worker3["Worker ..."]
        Worker8["Worker 8: CyberRangeEnv"]
    end

    TPE -->|Reward Parameters| Worker1 & Worker2 & Worker8
    Worker1 & Worker2 & Worker8 -->|Telemetry + Reward| Obs_Net
    Obs_Net -->|12D Latent State| Agent_Net
    Agent_Net -->|Mitigation Action| Worker1 & Worker2 & Worker8
    Worker1 & Worker2 & Worker8 -->|Experience Tuples| Replay
    Replay -->|Batch Updates| Agent_Net
    Worker1 & Worker2 & Worker8 -->|Trial Fitness| TPE
```
### Operational Logic: The Distributed Feedback Loop
The diagram above illustrates the **Async-Synchronous Hybrid Model** used in V5.
1.  **Distributed Sampling**: The `CyberRangeEnv` instances (Workers 1-8) run on the CPU. They handle the complex physics of network simulation (packet flows, service states). They do *not* run the neural networks.
2.  **Centralized Inference**: When a Worker needs a decision, it sends the raw observation `(Logs, Alerts, Metrics)` to the GPU-resident `Observer` and `Agent`. This prevents VRAM fragmentation (avoiding 8 copies of the model).
3.  **Optuna Tuning**: Indepedently, the `Optuna Optimization Kernel` watches the total reward of completed episodes. If a specific reward configuration (e.g., "Penalty=5.0") yields higher scores, it instructs the Workers to adopt these new parameters for future episodes. This creates a "Meta-Learning" loop where the *definition of success* evolves alongside the Agent's skill.

### 2.2 The "Perception-Action" Cycle (Sequence Diagram)
```mermaid
sequenceDiagram
    participant World as Environment (CPU)
    participant Observer as Deep Eyes (Observer)
    participant Agent as Deep Brain (Policy)
    participant Buffer as Replay Buffer
    
    Note over World, Observer: Phase 1: Perception
    World->>Observer: Raw Logs (Auth, Apache) + Telemetry
    Observer->>Observer: Tokenize & Embed (Bi-LSTM)
    Observer->>Observer: Attention Pooling
    Observer->>Agent: 12D Latent State Vector (Z-Scored)
    
    Note over Agent, World: Phase 2: Decision
    Agent->>Agent: Forward Pass (3x 512 Layers)
    Agent->>Agent: Apply Valid Action Mask
    Agent->>World: Action ID (e.g., 221 Block IP)
    
    Note over World, Buffer: Phase 3: Effect
    World->>World: Resolve Physics (Mitigation)
    World->>Agent: Reward Signal (R_t)
    World->>Buffer: Store Tuple (s, a, r, s', done)
```
### Operational Logic: From Chaos to Order
The sequence diagram details the transformation of unstructured chaos into structured decisions:
1.  **Phase 1 (Perception)**: The World generates messy string data ("Failed password..."). The **Observer** acts as a funnel. It uses Bi-LSTM layers to "read" these logs and condense them into a clean **12D Vector**. This vector is the only thing the Agent ever sees. If the Observer fails (Blindness), the Agent is helpless.
2.  **Phase 2 (Decision)**: The Agent receives the 12D vector. It doesn't know it came from logs. It just sees `Risk=0.9` and `Confidence=0.8`. It runs this through its deep neural network to select an action. Crucially, the **Action Mask** prevents illegal moves (e.g., it cannot "Unblock IP" if no IP is blocked).
3.  **Phase 3 (Effect)**: The action is executed in the simulator. The system calculates a **Reward ($R_t$)** based on the outcome (Did the attack stop? Did we break a service?). This experience tuple is stored in the buffer for PPO updates.

---

## 3. Component Deep-Dive: The Observer ("Deep Eyes") üëÅÔ∏è

The Observer is the system's perception engine. Its job is **Dimensionality Reduction** and **Anomaly Detection**. In V5, we solved the "Early-Level Blindness" issue by upgrading to a **3-Layer Deep Head** trained with **Contrastive Bootstrapping**.

### 3.1 Observer Architecture
```mermaid
graph TD
    Input_Logs["Log Text (Seq: 32)"]
    Input_Alerts["Snort Alert ID"]
    Input_Metrics["Device Metrics"]

    subgraph "Feature Extraction"
        Input_Logs -->|Bi-LSTM (64 units)| Feat_Text["Temporal Context"]
        Input_Alerts -->|Embedding (16D)| Feat_Alert["Semantic ID"]
        Input_Metrics -->|Log1p + Linear| Feat_Metric["Scaled Telemetry"]
    end

    subgraph "Fusion & Deep Reasoning"
        Feat_Text & Feat_Alert & Feat_Metric -->|Concatenate| Integrated["Joint Vector (88D)"]
        Integrated -->|Relu| L1["Layer 1 (256)"]
        L1 -->|Relu| L2["Layer 2 (128)"]
        L2 -->|LayerNorm| Latent["Latent State (12D)"]
    end

    subgraph "Training Objectives"
        Latent -->|Decoder| Recon["Reconstruction Loss (MSE)"]
        Latent -->|Semantic Guard| Signal["3D Signal: [Risk, Conf, Sev]"]
        Signal -->|BCE Loss| GroundTruth["Attack Labels"]
    end
```
### Operational Logic: The Fusion Core
The Observer is a **Multi-Modal Autoencoder**. It fuses three incompatible data types:
1.  **Text (Logs)**: Processed by Bi-LSTMs to extract temporal patterns (e.g., "5 failed logins in 1 second").
2.  **Categorical (Alerts)**: Processed by Entity Embeddings ($16D$) to learn the semantic meaning of Snort IDs (e.g., learning that `ET SCAN` is related to `ET EXPLOIT`).
3.  **Continuous (Metrics)**: Processed by Log-Scaling to handle the vast dynamic range of network traffic (0 bps vs 10Gbps).
These features are concatenated into a **Joint Vector (88D)** and compressed down to **12D**. The **Semantic Guard** ensures this compressed representation retains the crucial "Risk Score", preventing the model from just learning to copy input to output (Identity Mapping).

### 3.2 Detailed Logic: Multi-Modal Attention
The `MultiModalEncoder` uses a dedicated Attention Mechanism to handle variable-length log sequences (e.g., 5 logs vs 500 logs).

```mermaid
graph LR
    Input["Input Sequence (N x 64)"] --> LSTM[Bi-LSTM Layer]
    LSTM --> Hidden["Hidden States (N x 128)"]
    
    subgraph "Attention Mechanism"
        Hidden --> Attn["Attention Linear Layer"]
        Attn --> Scores["Alpha Scores (N x 1)"]
        Scores --> Softmax["Softmax"]
        Softmax --> Weights["Attention Weights"]
    end
    
    Weights & Hidden --> Dot["Weighted Sum"]
    Dot --> Context["Context Vector (128)"]
    Context --> Output["Linear Projection (64)"]
```
### Operational Logic: Why Attention Matters
In a Cyber Attack, 99% of logs are noise (benign traffic). Only 1% (the specific exploit attempt) matters.
*   **Without Attention**: An LSTM averaging all time steps would dilute the exploit signal with the noise, causing "Signal Washout."
*   **With Attention**: The Attention Mechanism assigns a weight $\alpha_i$ to each log entry. The network learns to assign $\alpha \approx 1.0$ to "Failed password" and $\alpha \approx 0.0$ to "User logged out." This allows the Observer to "zoom in" on the needle in the haystack, creating a robust context vector even in noisy environments.

### 3.3 Data Specifications (Inputs) üìù

The Observer ingests three distinct data streams. Below are the **exact schemas** used in V5.

#### A. Raw Logs (Sequence Length: 50)
Generated by `simulator.evidence.EvidenceGenerator`.
| Log Type | Trigger Condition | Template Format |
| :--- | :--- | :--- |
| **Auth** | Bruteforce Attack | `Failed password for root from {src_ip} port {port}` |
| **Apache** | Web Attack (Risk > 0.4) | `GET /cgi-bin/vulnerable.sh?cmd=whoami from {src_ip}` |
| **MQTT** | IoT Scan (Port 1883) | `Client {src_ip} connected to MQTT broker` |

**Tokenization**:
Strings are hashed using a rolling hash: $h = (31 \times h + \text{ord}(c)) \& \text{0xFFFFFFFF}$.
The final value is normalized: $v = (h \pmod{1000}) / 1000.0$.

#### B. Suricata Alerts (Sequence Length: 100)
| Attack Type | Signature ID | Severity | Template |
| :--- | :--- | :--- | :--- |
| **Port Scan** | `ET SCAN` | 2 | `ET SCAN Potential Port Scan` |
| **Bruteforce** | `ET EXPLOIT` | 3 | `ET EXPLOIT {service} Brute Force Attempt` |
| **DoS** | `ET DOS` | 3 | `ET DOS Potential DDoS Activity Detected` |

#### C. Device Telemetry (32-Dim Vector)
Key metrics prioritized in the input vector:
1.  `cpu_percent` (Normalized 0-1)
2.  `mem_percent` (Normalized 0-1)
3.  `tx_bps` (Log-scaled: $\log(1+x)/10$)
4.  `rx_bps` (Log-scaled: $\log(1+x)/10$)
5.  `active_conns` (Log-scaled)
6.  `unique_dst_ports_1m` (Log-scaled)

---

## 4. Component Deep-Dive: The Agent ("Deep Brain") üß†

The Agent uses **Proximal Policy Optimization (PPO)** to learn strategy. It treats the Cyber Range as a Partially Observable Markov Decision Process (POMDP). It does not know *how* to block an IP, only *that* it should block it to get a reward.

### 4.1 Agent Neural Architecture (V5)
The V5 Agent is significantly deeper (3 Shared Layers) than V3 to capture complex reasoning patterns.

```mermaid
graph TD
    State["12D Input State"]

    subgraph "Shared Perception (V5: 512 Units)"
        State -->|Linear 512| L1["Layer 1 (512)"]
        L1 -->|ReLU| A1["Activation"]
        A1 -->|Linear 512| L2["Layer 2 (512)"]
        L2 -->|ReLU| A2["Activation"]
        A2 -->|Linear 512| L3["Layer 3 (512 - Extra Depth)"]
        L3 -->|ReLU| A3["Activation"]
    end

    subgraph "The Actor (Policy Head)"
        A3 -->|Linear 256| LA["Actor Latent (256)"]
        LA -->|ReLU| AA["Activation"]
        AA -->|Linear Out| Logits["Unnormalized Logits (Action Dim)"]
        Logits -->|Action Mask| Masked["Valid Logits"]
        Masked -->|Softmax| Prob["Probabilities"]
        Prob -->|Categorical Sample| Action["Selected Action"]
    end

    subgraph "The Critic (Value Head)"
        A3 -->|Linear 256| LC["Critic Latent (256)"]
        LC -->|ReLU| AC["Activation"]
        AC -->|Linear 1| Value["Predicted Value V(s)"]
    end
```
### Operational Logic: Deep Reasoning
The Brain is split into three parts:
1.  **Shared Perception (3 Layers)**: This deep stack of 512 neurons allows the agent to build higher-order concepts from the 12D input. For example, it learns that "High Risk + Zero Confidence" usually means "Unknown Zero-Day." The extra depth in V5 (Layer 3) was key to solving the Level 3 (Persistence) challenge.
2.  **The Actor (Policy Head)**: This head outputs probabilities for action. It asks: *"Given the state, what should I do?"* It uses a Softmax distribution to sample actions, allowing for exploration.
3.  **The Critic (Value Head)**: This head outputs a single number: $V(s)$. It asks: *"Step-by-step, how good is this situation?"* This value estimate stabilizes training by providing a baseline to compare rewards against, reducing variance.

### 4.2 The 12-Dimensional State Vector
Using **Online Z-Score Normalization**, raw metrics are converted to a standard normal distribution ($\mu=0, \sigma=1$).

| Index | Field | Description | Range (Normalized) |
| :--- | :--- | :--- | :--- |
| **0** | `incident_score` | Explicit Risk from Deep Eyes | [0.0, 1.0] |
| **1** | `incident_confidence` | Model certainty | [0.0, 1.0] |
| **2** | `severity_level` | Attack Severity (0-3) | [0.0, 1.0] |
| **3** | `asset_criticality` | Target Importance | [-3.0, 3.0] |
| **4** | `zone_dmz` | 1=DMZ, 0=Internal | [Binary] |
| **5-11** | `metrics` | CPU, RAM, BPS, PPS, Ports, Conns, Iso | [-3.0, 3.0] |

### 4.3 The Action Space (Hierarchical V2)
Actions are grouped into Tiers based on disruption cost.

| Tier | Action Type | Examples | Cost | Disruption |
| :--- | :--- | :--- | :--- | :--- |
| **0** | **Monitor** | `Wait`, `Monitor` | 0.00 | None |
| **1** | **Investigate** | `Increase Logging`, `PCAP Capture` | 0.05 | Low |
| **2** | **Contain** | `Block IP`, `Rate Limit` | 0.15 | Medium |
| **3** | **Recover** | `Reimage`, `Shutdown` | 0.35 | High |

---

## 5. Component Deep-Dive: The Optimization Kernel (Optuna) üß¨

In V5, we replaced manual reward tuning with **Bayesian Optimization**. A centralized Optuna study runs alongside the PPO training, constantly adjusting the reward function coefficients to maximize learning speed.

### 5.1 Optimization Loop Diagram
```mermaid
graph LR
    History["Training History"] -->|TPE Analysis| Suggest["Suggest Params"]
    Suggest -->|Coefficients| Worker["Environment Worker"]
    Worker -->|Episode Reward| Evaluate["Evaluate Performance"]
    Evaluate -->|Update| History
```
### Operational Logic: The Feedback Loop
This is the "Brain's Trainer."
1.  **Suggest Params**: Optuna uses the Tree-Structured Parzen Estimator (TPE) to guess a set of reward weights (e.g., "Punish False Positives by 2.0x").
2.  **Evaluate**: The workers run an episode with these weights.
3.  **Update**: If the Agent fails (Negative Reward), Optuna learns that this weight combination is bad. If the Agent succeeds (High Positive Reward), Optuna zooms in on that parameters space.
This autonomous tuning removed the need for weeks of manual "Reward Shaping" by human engineers.

### 5.2 Tuned Reward Parameters
Optuna converged on the following optimal coefficients after 768 trials:
*   **Risk Penalty**: **2.5x** (Strong aversion to high risk)
*   **Action Cost**: **0.8x** (Encourages active defense)
*   **Stall Penalty**: **4.64x** (Strong punishment for inaction during attacks)
*   **False Positive Penalty**: **1.91x** (Balance between security and continuity)

---

## 6. Historical Evolution: V1 to V5 üìú

The system has evolved through five major architectural revisions, each solving specific failures.

### 6.1 Version Comparison Table
| Feature | V1 (Prototype) | V3 (Legacy) | V5 (Deep Eyes) | Status |
| :--- | :--- | :--- | :--- | :--- |
| **Observer Architecture** | 1-Layer (Collapsed) | 2-Layer (Shallow) | **3-Layer "Deep Eyes"** | ‚úÖ Solved Capacity |
| **Latent Dims** | 12D (Zero Variance) | 12D (Low Variance) | **12D (High Variance)** | ‚úÖ Solved Blindness |
| **Bootstrap Method** | Standard Supervised | Noisy Simulation | **Contrastive Synthetic** | ‚úÖ Solved Noise |
| **Perception Delta** | < 0.01 (Blind) | 0.05 | **0.94 (Actionable)** | ‚úÖ Solved Signal |
| **Agent Net** | 128 Units | 256 Units | **512 Units (Deep)** | ‚úÖ Solved Complexity |
| **Training Speed** | 40 steps/sec | 120 steps/sec | **850 steps/sec** | ‚úÖ Solved Speed |
| **Reward Tuning** | Manual Guesswork | Genetic Algo | **Autonomous Optuna** | ‚úÖ Solved Tuning |

### 6.2 The "Latent Collapse" Post-Mortem (V1/V2)
In V1, the model's loss would decrease (implying learning), yet the Agent's performance remained 0%.
*   **Evidence**: The 12D vector showed values like `[0.89, 0.91, 0.90...]`.
*   **Root Cause**: The "Average Noise" of the network overwhelmed the subtle "Attack Signal." The Autoencoder learned to just output the average.
*   **V5 Solution**: We introduced **Embedding Separation Loss**, which explicitly penalizes the model if the "Attack Vector" is closer than distance $\delta=0.5$ to the "Benign Vector."

---

## 7. The Training Pipeline & Curriculum üõ§Ô∏è

Training follows a strict **Curriculum Learning** path. The Agent must prove mastery at each level (90% success rate) before the environments are "promoted."

### 7.1 Curriculum Levels
| Level | Name | Devices | Steps | Complexity Focus |
| :--- | :--- | :--- | :--- | :--- |
| **0** | **Bootstrap** | 10 | 300 | Simple Scans & Brute Force |
| **1** | **Scaling Up** | 50 | 500 | Credential Spraying Campaigns |
| **2** | **Complexity** | 100 | 800 | Lateral Movement & Pivot Chains |
| **3** | **Advanced** | 300 | 1,500 | Ransomware Bursts & Persistence |
| **4** | **Production** | 500 | 2,500 | Full-Scale APT Campaigns |

---

## 8. Validated Evaluation: The Ultimate Diagnostic üìä

The **Ultimate Deep Diagnostic** ran on the final V5 checkpoint (Phase 6 Alignment). These results constitute the **Proof of Alignment**.

### 8.1 Production Scale Performance (Level 4)
*   **Perception Fidelity**:
    *   Benign Risk: **0.018**
    *   Attack Risk: **0.960**
    *   **Risk Delta**: **0.942** (Target: >0.5) ‚úÖ **EXCEEDS SPEC**
*   **Action Strategy**:
    *   Diversity: **Healthy** (Mix of `Increase Logging` and `Block IP`)
    *   Policy Entropy: **0.08** (Confident but adaptive)
*   **Mitigation Efficacy**:
    *   Attack Survival: **100%**
    *   Mean Time to Contain: **3 Steps**
*   **Reward Dynamics**:
    *   Avg Reward (Attack): **+140.60**
    *   Avg Reward (Benign): **+0.05**

**Verdict**: The V5 Agent correctly identifies high-risk states and deploys appropriate countermeasures without hesitation. The "Blindness" bugs of V3 are completely resolved.

---

## 9. Operational Guide üíª

### 9.1 Training Commands
To launch a full Training Run (V5 Distributed):
```bash
# Clean start (removes old logs)
rm -rf logs/ppo_training/*

# Launch Distributed Training (4 Envs)
PYTHONPATH=. .venv/bin/python train/train_ppo.py \
    --steps 5000000 \
    --n_envs 4 \
    --lr 3e-4 \
    --id "v5_distributed_run"
```

### 9.2 Evaluation Commands
To run the "Ultimate Deep Diagnostic":
```bash
# Run exhaustive diagnostic
PYTHONPATH=. .venv/bin/python eval/ultimate_deep_diagnostic.py \
    --model logs/ppo_training/<RUN_ID>/final_model.pth \
    --episodes 10
```

### 9.3 Fine-Tuning Commands
To fine-tune a specific model on a new attack type:
```bash
PYTHONPATH=. .venv/bin/python train/train_iterative.py \
    --base_model models/agent/v5_agent.pth \
    --epochs 5 \
    --lr 1e-5
```

---

## 10. Production Safeguards üõ°Ô∏è

To ensure stability in long-running (300k+ step) trainings, we implemented specific engineering safeguards.

1.  **Crash Prevention**: `env.step()` is wrapped in a `try-except` block. If a physics error occurs, it returns a "Safe Fallback" (Wait Action) instead of crashing the training cluster.
2.  **Manifest Verification**: PPO cannot start without a cryptographically signed `observer_manifest.json` ensuring it is using the correct "Eyes."
3.  **Numerical Watchdogs**: A background monitor checks for `NaN` or `Inf` values in the latent vector and auto-halts training if "Latent Collapse" (Zero Variance) is detected.
4.  **Log Rotation**: The `SimulationLogger` uses 4 modes. `EPISODE_SUMMARY` (Default) reduces disk usage by 98% compared to V3.

---

## 11. Future Roadmap üîÆ

With V5 stable, the roadmap focuses on Explainability and Human Trust.

### Phase 7: Explainable AI (XAI)
*   **Goal**: Humans need to know *why* the Agent blocked an IP.
*   **Tech**: Implement **Integrated Gradients** or **SHAP** values on the `12D State Vector` to highlight which feature (e.g., "High PPS") triggered the decision.

### Phase 8: Federated Defense
*   **Goal**: Multiple Agents defending different subnets.
*   **Tech**: **Multi-Agent PPO (MAPPO)** sharing a central Value Function but independent Policies.

---

**Document Version**: 5.4.0 (Magnum Opus: Explained Diagrams Edition)
**Generated By**: Antigravity (Google DeepMind)
